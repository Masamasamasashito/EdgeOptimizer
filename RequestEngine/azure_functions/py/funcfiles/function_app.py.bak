# ----------------------------------------------------
# Edge Optimizer
# Request Engine for Azure Functions
# Crafted by Nishi Labo | https://4649-24.com
# ---------------------------------------------------
# Dummy comment for workflow verification
# 
# EO Request Engine for Azure Functions japan-east region
# * Overview:
# Executes HTTP requests to specified URLs to warm up the Azure Functions cache.
# Returns results in a flat JSON structure (supporting TTFB/BodySize measurements).
# 
# * Input:
# - HTTP Method: POST (GET is not implemented)
# - JSON Body: { data: { targetUrl, tokenCalculatedByN8n, headers, httpRequestNumber, httpRequestUUID, httpRequestRoundID } }
#   * targetUrl: Target URL to warm up (required)
#   * tokenCalculatedByN8n: SHA-256(url + リクエストシークレット) calculated in n8n using EO_Infra_Docker/.env N8N_EO_REQUEST_SECRET (required)
#   * headers: Optional custom headers (object)
#   * httpRequestNumber: Optional request sequence number
#   * httpRequestUUID: Optional UUID for each request (created by n8n)
#   * httpRequestRoundID: Optional UNIX timestamp when the first request of a round reaches 215 Step.2 Separate Starter Dummy (created by n8n) - 最初に215 Step.2 Separate Starter Dummyに到達した時点のUNIXタイムスタンプをそのラウンド1回の中の全リクエストに入れる
# - Actual usage: n8n workflow (EOn8nWorkflowJson/eo-xmlsitemap-jp.json) sends POST with JSON body
# 
# * Dependencies:
# - Azure Key Vault (env.EO_AZ_RE_KEYVAULT_URL) - Key Vault URI
# - Azure Key Vault Secret Name: AZFUNC-REQUEST-SECRET (AZFUNC_REQUEST_SECRET_NAME)
#
# * Logging Policy (ログ方針):
# EOアーキテクチャに基づくログ方針:
# - 正規ログ経路（推奨）: stdout → Application Insights → Log Sink → BigQuery
#   * Application Insightsを有効化した場合、標準出力（loggingモジュール）が自動的にApplication Insightsに送信されます
#   * Application InsightsはAzure Functionsの統合監視機能として、パフォーマンス・エラー・依存関係のテレメトリを収集します
#   * レイテンシを考慮し、各クラウドのネイティブログ基盤（AzureはApplication Insights）を経由します
# - 簡易ログ経路: stdout → Geminiで分析（ネイティブログ基盤を経由しない）
#   * Application Insightsを無効化した場合、標準出力のみを使用します
# 本実装では、Python標準のloggingモジュールを使用しており、Application Insightsの有効/無効に関わらず動作します。
# Application Insightsを有効にする場合は、Function App作成時または設定で有効化してください。

import os
import time
import hashlib
import json
import logging
import re
import ssl
from typing import Any, Dict, Optional, Tuple
from urllib.parse import urlparse

import requests
import azure.functions as func
from azure.identity import DefaultAzureCredential
from azure.keyvault.secrets import SecretClient

# ======================================================================
# 設定定数
# ======================================================================

# Azure Key Vault 設定
# キーコンテナURI > GitHubシークレット > EO_AZ_RE_KEYVAULT_URL に記載しているので、ここではシークレット名のみを定義する
# Secret Name of Azure Key Vault (Not Key Vault Name)
AZFUNC_REQUEST_SECRET_NAME = "AZFUNC-REQUEST-SECRET"  # Is not a Key Vault Name, but a Secret Name of Azure Key Vault

# EO識別ヘッダー
# Request Engineの識別用に使用されるカスタムヘッダー
EO_HEADER_NAME = "x-eo-re"
EO_HEADER_VALUE = "azure"

# ======================================================================
# パフォーマンス最適化設定
# ======================================================================
HTTP_REQUEST_TIMEOUT = 10  # Warmup目的なら10秒で十分（従来は30秒）
MAX_CONTENT_SIZE_FOR_ANALYSIS = 5 * 1024 * 1024  # 5MB以上は解析をスキップ（メモリ保護）

# ======================================================================
# リトライ設定
# ======================================================================
MAX_RETRY_ATTEMPTS = 2  # 最大リトライ回数（初回含めて合計3回）
RETRY_INITIAL_DELAY = 0.5  # 初回リトライまでの待機時間（秒）
RETRY_BACKOFF_MULTIPLIER = 2.0  # エクスポネンシャルバックオフの倍率
RETRYABLE_STATUS_CODES = {500, 502, 503, 504}  # リトライ対象のHTTPステータスコード

# ======================================================================
# グローバル変数
# ======================================================================
_cached_kv_request_secret: Optional[str] = None

app = func.FunctionApp()


# ======================================================================
# 実行環境識別子（Azure Functions実行ID）の取得
# ======================================================================
def _get_execution_id() -> Optional[str]:
    """
    Azure Functionsの実行IDを取得
    環境変数から取得を試みる（取得できない場合はNoneを返す）
    """
    # Azure Functionsの実行IDは環境変数では直接取得できない可能性がある
    # 将来的にAzure Functions SDKから取得できるようになった場合に備えて、環境変数をチェック
    execution_id = os.environ.get("_X_MS_EXECUTION_ID") or os.environ.get("WEBSITE_INSTANCE_ID")
    return execution_id if execution_id else None

# ======================================================================
# Azure Key Vault から secret の AZFUNC-REQUEST-SECRET 取得（1回だけ取得、以降キャッシュ）
# ======================================================================
def _get_kv_request_secret() -> str:
    global _cached_kv_request_secret
    if _cached_kv_request_secret is not None:
        return _cached_kv_request_secret
    # EO_AZ_RE_KEYVAULT_URL from Function App application settings (set via GitHub Actions or Azure Portal)
    keyvault_url = os.environ.get("EO_AZ_RE_KEYVAULT_URL")
    if not keyvault_url:
        raise RuntimeError("EO_AZ_RE_KEYVAULT_URL environment variable is not set")

    try:
        # 
        azure_credential = DefaultAzureCredential()
        azure_kv_secret_client = SecretClient(vault_url=keyvault_url, credential=azure_credential)
        azure_kv_secret = azure_kv_secret_client.get_secret(AZFUNC_REQUEST_SECRET_NAME)
        _cached_kv_request_secret = azure_kv_secret.value
        return _cached_kv_request_secret
    except Exception as e:
        raise RuntimeError(f"Failed to get secret from Azure Key Vault: {str(e)}")


# ======================================================================
# token 計算 (SHA-256): SHA-256(url + リクエストシークレット)
# リクエストシークレットは Azure Key Vault の AZFUNC-REQUEST-SECRET から取得（n8n側では N8N_EO_REQUEST_SECRET を使用）
# ======================================================================
def _calc_token(url: str, secret: str) -> str:
    return hashlib.sha256(f"{url}{secret}".encode()).hexdigest()


# ======================================================================
# セキュリティヘッダーを解析して指標を返す
# ======================================================================
def _analyze_security_headers(res_headers: Dict[str, str], target_url: str) -> Dict[str, Any]:
    """
    セキュリティヘッダーを解析して指標を返す
    Core Web Vitals / PageSpeed Insights / セキュリティ指標に含まれる
    """
    security = {}
    security["is_https"] = target_url.startswith("https://")
    
    headers_lower = {k.lower(): v for k, v in res_headers.items()}
    
    # セキュリティヘッダー定義（ヘッダー名 -> キー名のマッピング）
    security_headers = {
        "strict-transport-security": "hsts",
        "content-security-policy": "csp",
        "x-content-type-options": "x_content_type_options",
        "x-frame-options": "x_frame_options",
        "x-xss-protection": "x_xss_protection",
        "referrer-policy": "referrer_policy",
        "public-key-pins": "hpkp",
    }
    
    # 各セキュリティヘッダーをチェック
    for header_name, key_prefix in security_headers.items():
        header_value = headers_lower.get(header_name)
        security[f"{key_prefix}_present"] = header_value is not None
        if header_value:
            security[f"{key_prefix}_value"] = header_value
    
    # Permissions-Policy (旧Feature-Policy) - 特殊処理
    permissions_policy = headers_lower.get("permissions-policy") or headers_lower.get("feature-policy")
    security["permissions_policy_present"] = permissions_policy is not None
    if permissions_policy:
        security["permissions_policy_value"] = permissions_policy
    
    return security


# ======================================================================
# HTTPプロトコルバージョン取得
# ======================================================================
def _get_http_protocol_version(response: requests.Response) -> Optional[str]:
    """
    HTTPプロトコルバージョンを取得（HTTP/1.1, HTTP/2, HTTP/3など）
    判定が困難な場合は理由を含むエラーメッセージを返す
    """
    if not hasattr(response, 'raw') or response.raw is None:
        return "ERROR: Cannot determine HTTP protocol version. The response object does not have a 'raw' attribute or it is None. The 'raw' attribute is required to access the underlying HTTP version information."
    
    try:
        version = getattr(response.raw, 'version', None)
        if version is None:
            version = getattr(response.raw, '_http_version', None)
        if version:
            if version == 11:
                return "HTTP/1.1"
            elif version == 20:
                return "HTTP/2"
            elif version == 30:
                return "HTTP/3"
            elif isinstance(version, (int, float)):
                return f"HTTP/{version/10}"
            else:
                return str(version)
        else:
            return "ERROR: Cannot determine HTTP protocol version. The response.raw object does not have 'version' or '_http_version' attributes. These attributes are required to determine the HTTP protocol version, but they are not available in this response object."
    except AttributeError as e:
        return f"ERROR: Cannot determine HTTP protocol version due to AttributeError. Accessing the HTTP version attribute failed: {str(e)}. The response object structure may not match the expected format."
    except TypeError as e:
        return f"ERROR: Cannot determine HTTP protocol version due to TypeError. Type mismatch occurred while accessing the HTTP version: {str(e)}."
    except Exception as e:
        return f"ERROR: Cannot determine HTTP protocol version due to unexpected error ({type(e).__name__}): {str(e)}."


# ======================================================================
# TLSバージョン取得
# ======================================================================
def _get_tls_version(response: requests.Response, target_url: str) -> Optional[str]:
    """
    TLSバージョンを取得
    失敗時は理由を含む値を返す
    """
    if not target_url.startswith("https://"):
        return "unknown: not_https"
    if not hasattr(response, 'raw') or response.raw is None:
        return "unknown: response_raw_not_available"
    
    try:
        connection = getattr(response.raw, '_connection', None)
        if connection is None:
            return "unknown: connection_not_found"
        
        sock = getattr(connection, 'sock', None)
        if sock is None:
            return "unknown: sock_not_found"
        if not isinstance(sock, ssl.SSLSocket):
            return f"unknown: sock_not_ssl (type: {type(sock).__name__})"
        
        try:
            ssl_version_str = sock.version()
            tls_version_map = {
                'TLSv1': 'TLSv1.0',
                'TLSv1.1': 'TLSv1.1',
                'TLSv1.2': 'TLSv1.2',
                'TLSv1.3': 'TLSv1.3',
            }
            for key, value in tls_version_map.items():
                if key in ssl_version_str:
                    return value
            if ssl_version_str:
                return ssl_version_str
            return "unknown: version_string_empty"
        except AttributeError as e:
            return f"unknown: version_method_failed (AttributeError: {str(e)})"
        except TypeError as e:
            return f"unknown: version_method_failed (TypeError: {str(e)})"
        except Exception as e:
            return f"unknown: version_method_failed (Exception: {type(e).__name__}: {str(e)})"
    except AttributeError as e:
        return f"unknown: connection_access_failed (AttributeError: {str(e)})"
    except TypeError as e:
        return f"unknown: connection_access_failed (TypeError: {str(e)})"
    except Exception as e:
        return f"unknown: connection_access_failed (Exception: {type(e).__name__}: {str(e)})"


# ======================================================================
# Core Web Vitals: FCP (First Contentful Paint) 近似値計測
# ======================================================================
def _measure_fcp_approximation(
    response: requests.Response,
    request_start_time: float,
) -> Tuple[Dict[str, Any], bytes]:
    """
    FCP (First Contentful Paint) の近似値を計測
    サーバー側では直接計測できないため、以下の指標を計測：
    - fcp_14kb_ms: 最初の14KB（TCP初期ウィンドウサイズ）の読み込み時間
    - fcp_50kb_ms: 最初の50KB（一般的なFCP閾値）の読み込み時間
    
    Returns:
        Tuple[Dict[str, Any], bytes]: (FCPメトリクス, 読み込んだ全コンテンツ)
    """
    fcp_metrics = {}
    full_content = b""
    fcp_14kb_measured = False
    fcp_50kb_measured = False
    
    try:
        target_size_14kb = 14 * 1024  # 14KB
        target_size_50kb = 50 * 1024  # 50KB
        
        # stream=Trueで段階的に読み込む
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                full_content += chunk
                
                # 14KBの計測
                if not fcp_14kb_measured and len(full_content) >= target_size_14kb:
                    chunk_14kb_end = time.time()
                    fcp_14kb_ms = (chunk_14kb_end - request_start_time) * 1000
                    fcp_metrics["fcp_14kb_ms"] = round(fcp_14kb_ms, 2)
                    fcp_metrics["fcp_14kb_bytes"] = len(full_content)
                    fcp_14kb_measured = True
                
                # 50KBの計測
                if not fcp_50kb_measured and len(full_content) >= target_size_50kb:
                    chunk_50kb_end = time.time()
                    fcp_50kb_ms = (chunk_50kb_end - request_start_time) * 1000
                    fcp_metrics["fcp_50kb_ms"] = round(fcp_50kb_ms, 2)
                    fcp_metrics["fcp_50kb_bytes"] = len(full_content)
                    fcp_50kb_measured = True
                
                # 両方計測済みで、十分なデータが読み込まれた場合は早期終了可能
                # ただし、全コンテンツを読み込む必要があるため、続行
        
        # 50KB未満で終了した場合
        if not fcp_50kb_measured and len(full_content) > 0:
            chunk_50kb_end = time.time()
            fcp_50kb_ms = (chunk_50kb_end - request_start_time) * 1000
            fcp_metrics["fcp_50kb_ms"] = round(fcp_50kb_ms, 2)
            fcp_metrics["fcp_50kb_bytes"] = len(full_content)
            fcp_metrics["fcp_50kb_incomplete"] = True
        
        # 14KB未満で終了した場合
        if not fcp_14kb_measured and len(full_content) > 0:
            chunk_14kb_end = time.time()
            fcp_14kb_ms = (chunk_14kb_end - request_start_time) * 1000
            fcp_metrics["fcp_14kb_ms"] = round(fcp_14kb_ms, 2)
            fcp_metrics["fcp_14kb_bytes"] = len(full_content)
            fcp_metrics["fcp_14kb_incomplete"] = True
        
    except Exception as e:
        logging.warning(f"FCP approximation measurement error: {str(e)}")
        fcp_metrics["fcp_measurement_error"] = str(e)
        # エラーが発生しても、読み込めた部分は返す
    
    return fcp_metrics, full_content


# ======================================================================
# URL拡張子の判定
# ======================================================================
def _get_url_extension(url: str) -> Optional[str]:
    """
    URLから拡張子を取得（クエリパラメータとアンカーを除去）
    """
    try:
        parsed = urlparse(url)
        path = parsed.path
        if not path:
            return None
        # 拡張子を抽出（最後の.以降）
        if '.' in path:
            ext = path.rsplit('.', 1)[-1].lower()
            # 拡張子が妥当な文字列かチェック（英数字のみ）
            if ext and ext.isalnum():
                return ext
        return None
    except Exception:
        return None


# ======================================================================
# URLタイプと拡張子に基づくリソース種別の判定
# ======================================================================
def _determine_resource_type(urltype: Optional[str], url: str) -> Dict[str, Any]:
    """
    urltypeとURL拡張子からリソース種別を判定
    
    n8nワークフロー（160ノード: Asset / MainDoc / Exception Classification）と整合性を保つ
    
    アセット拡張子:
    - 画像: jpg, jpeg, gif, png, webp, avif, svg, ico
    - CSS: css
    - JS: js
    - フォント: woff, woff2, ttf, otf, eot
    - 動画: mp4, webm, ogg, mov
    
    resource_category:
    - "html", "image", "css", "js", "font", "video", "other"
    """
    resource_info = {
        "urltype": urltype,
        "url_extension": None,  # 拡張子（例: "woff2", "ttf", "css", "jpg"）
        "resource_category": None,  # "html", "image", "css", "js", "font", "video", "other"
        "is_main_document": False,
        "is_asset": False,
        "needs_fcp_measurement": False,
        "needs_lcp_analysis": False,
        "needs_asset_analysis": False,
    }
    
    # 拡張子を取得
    ext = _get_url_extension(url)
    resource_info["url_extension"] = ext
    
    # urltypeに基づく判定
    if urltype == "main_document":
        resource_info["is_main_document"] = True
        resource_info["needs_fcp_measurement"] = True
        resource_info["needs_lcp_analysis"] = True
        # 拡張子が.html/.htmの場合は確実にHTML
        if ext in ["html", "htm"]:
            resource_info["resource_category"] = "html"
        else:
            # 拡張子がない場合もHTMLの可能性が高い
            resource_info["resource_category"] = "html"
    
    elif urltype == "asset":
        resource_info["is_asset"] = True
        resource_info["needs_asset_analysis"] = True
        
        # 拡張子に基づく種別判定
        image_exts = ["jpg", "jpeg", "gif", "png", "webp", "avif", "svg", "ico"]
        css_exts = ["css"]
        js_exts = ["js"]
        font_exts = ["woff", "woff2", "ttf", "otf", "eot"]
        video_exts = ["mp4", "webm", "ogg", "mov"]
        
        if ext in image_exts:
            resource_info["resource_category"] = "image"
        elif ext in css_exts:
            resource_info["resource_category"] = "css"
        elif ext in js_exts:
            resource_info["resource_category"] = "js"
        elif ext in font_exts:
            resource_info["resource_category"] = "font"
        elif ext in video_exts:
            resource_info["resource_category"] = "video"
        else:
            resource_info["resource_category"] = "other"
    
    elif urltype == "exception":
        resource_info["resource_category"] = "exception"
        # exceptionは計測をスキップ
    
    return resource_info


# ======================================================================
# Core Web Vitals: LCP (Largest Contentful Paint) 要素の抽出
# ======================================================================
def _extract_lcp_elements(
    content: bytes,
    content_type: Optional[str],
    base_url: str,
    urltype: Optional[str] = None,
    url_extension: Optional[str] = None,
) -> Dict[str, Any]:
    """
    LCP (Largest Contentful Paint) の要素となるリソースを抽出
    HTMLをパースして、画像、動画、テキストブロックを特定
    
    urltypeと拡張子に基づいて、より適切な解析を行う
    """
    lcp_metrics = {}
    
    # urltypeがmain_documentでない場合は、HTML解析をスキップ
    if urltype != "main_document":
        # assetの場合でも、拡張子がhtmlの場合は解析する
        if url_extension not in ["html", "htm"]:
            lcp_metrics["lcp_analysis_skipped"] = True
            lcp_metrics["lcp_skip_reason"] = f"urltype_not_main_document (urltype={urltype})"
            return lcp_metrics
    
    # Content-TypeがHTMLでない場合も、urltypeがmain_documentなら解析を試みる
    is_html_by_content_type = content_type and "text/html" in content_type.lower()
    is_html_by_extension = url_extension in ["html", "htm"]
    is_html_by_urltype = urltype == "main_document"
    
    if not (is_html_by_content_type or is_html_by_extension or is_html_by_urltype):
        lcp_metrics["lcp_analysis_skipped"] = True
        lcp_metrics["lcp_skip_reason"] = "not_html"
        return lcp_metrics
    
    try:
        # HTMLをデコード（エンコーディングを推測）
        html_content = None
        encodings = ['utf-8', 'shift_jis', 'euc-jp', 'iso-2022-jp']
        
        for encoding in encodings:
            try:
                html_content = content.decode(encoding)
                lcp_metrics["html_encoding"] = encoding
                break
            except UnicodeDecodeError:
                continue
        
        if not html_content:
            lcp_metrics["lcp_analysis_skipped"] = True
            lcp_metrics["lcp_skip_reason"] = "encoding_detection_failed"
            return lcp_metrics
        
        # 簡易的なHTMLパース（正規表現を使用）
        # 画像要素の抽出
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\']'
        images = re.findall(img_pattern, html_content, re.IGNORECASE)
        lcp_metrics["lcp_image_count"] = len(images)
        if images:
            lcp_metrics["lcp_first_image_src"] = images[0]
        
        # 動画要素の抽出
        video_pattern = r'<video[^>]+src=["\']([^"\']+)["\']'
        videos = re.findall(video_pattern, html_content, re.IGNORECASE)
        lcp_metrics["lcp_video_count"] = len(videos)
        if videos:
            lcp_metrics["lcp_first_video_src"] = videos[0]
        
        # 背景画像の抽出（CSS内）
        bg_image_pattern = r'background-image:\s*url\(["\']?([^"\']+)["\']?\)'
        bg_images = re.findall(bg_image_pattern, html_content, re.IGNORECASE)
        lcp_metrics["lcp_bg_image_count"] = len(bg_images)
        
        # テキストブロックの抽出（h1, h2, pタグなど）
        text_block_pattern = r'<(h1|h2|h3|p|div)[^>]*>([^<]+)</(h1|h2|h3|p|div)>'
        text_blocks = re.findall(text_block_pattern, html_content, re.IGNORECASE)
        lcp_metrics["lcp_text_block_count"] = len(text_blocks)
        
        # LCP候補要素の総数
        lcp_metrics["lcp_candidate_count"] = (
            len(images) + len(videos) + len(bg_images) + len(text_blocks)
        )
        
    except Exception as e:
        logging.warning(f"LCP element extraction error: {str(e)}")
        lcp_metrics["lcp_analysis_error"] = str(e)
    
    return lcp_metrics


# ======================================================================
# Critical Path（レンダリングブロッキングリソース）の抽出
# ======================================================================
def _extract_critical_path(
    content: bytes,
    content_type: Optional[str],
    base_url: str,
    urltype: Optional[str] = None,
    url_extension: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Critical Path（レンダリングブロッキングリソース）を抽出
    
    HTMLをパースして、レンダリングをブロックするリソースを特定:
    - <link rel="stylesheet"> (レンダリングブロッキングCSS)
    - <script> (defer/async属性なしのレンダリングブロッキングJS)
    - <link rel="preload"> (プリロードリソース)
    """
    critical_path = {
        "blocking_css": [],
        "blocking_js": [],
        "non_blocking_js": [],
        "preload_resources": [],
    }
    
    # urltypeがmain_documentでない場合は、HTML解析をスキップ
    if urltype != "main_document":
        if url_extension not in ["html", "htm"]:
            critical_path["critical_path_analysis_skipped"] = True
            critical_path["critical_path_skip_reason"] = f"urltype_not_main_document (urltype={urltype})"
            return critical_path
    
    # Content-TypeがHTMLでない場合も、urltypeがmain_documentなら解析を試みる
    is_html_by_content_type = content_type and "text/html" in content_type.lower()
    is_html_by_extension = url_extension in ["html", "htm"]
    is_html_by_urltype = urltype == "main_document"
    
    if not (is_html_by_content_type or is_html_by_extension or is_html_by_urltype):
        critical_path["critical_path_analysis_skipped"] = True
        critical_path["critical_path_skip_reason"] = "not_html"
        return critical_path
    
    try:
        # HTMLをデコード（エンコーディングを推測）
        html_content = None
        encodings = ['utf-8', 'shift_jis', 'euc-jp', 'iso-2022-jp']
        
        for encoding in encodings:
            try:
                html_content = content.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        
        if not html_content:
            critical_path["critical_path_analysis_skipped"] = True
            critical_path["critical_path_skip_reason"] = "encoding_detection_failed"
            return critical_path
        
        # 相対パスを絶対パスに変換するヘルパー関数
        def resolve_url(url: str, base: str) -> str:
            """相対パスを絶対パスに変換"""
            if not url:
                return url
            if url.startswith(('http://', 'https://', '//')):
                return url
            try:
                from urllib.parse import urljoin
                return urljoin(base, url)
            except Exception:
                return url
        
        # <link rel="stylesheet">を抽出（レンダリングブロッキングCSS）
        css_pattern = r'<link[^>]+rel=["\']stylesheet["\'][^>]+href=["\']([^"\']+)["\']'
        css_matches = re.finditer(css_pattern, html_content, re.IGNORECASE)
        for i, match in enumerate(css_matches):
            css_url = match.group(1)
            absolute_url = resolve_url(css_url, base_url)
            critical_path["blocking_css"].append({
                "url": absolute_url,
                "relative_url": css_url,
                "order": i
            })
        
        # <script>タグを抽出（defer/async属性をチェック）
        script_pattern = r'<script([^>]*)>(.*?)</script>'
        script_matches = re.finditer(script_pattern, html_content, re.IGNORECASE | re.DOTALL)
        blocking_js_count = 0
        non_blocking_js_count = 0
        
        for script_match in script_matches:
            script_attrs = script_match.group(1)
            script_src_match = re.search(r'src=["\']([^"\']+)["\']', script_attrs, re.IGNORECASE)
            
            if script_src_match:
                script_url = script_src_match.group(1)
                absolute_url = resolve_url(script_url, base_url)
                
                # defer/async属性のチェック
                has_defer = 'defer' in script_attrs.lower()
                has_async = 'async' in script_attrs.lower()
                script_type_match = re.search(r'type=["\']([^"\']+)["\']', script_attrs, re.IGNORECASE)
                script_type = script_type_match.group(1).lower() if script_type_match else ""
                
                # moduleタイプのスクリプトは通常非ブロッキング
                is_module = script_type == "module"
                
                if has_defer or has_async or is_module:
                    critical_path["non_blocking_js"].append({
                        "url": absolute_url,
                        "relative_url": script_url,
                        "defer": has_defer,
                        "async": has_async,
                        "type": script_type if script_type else None,
                        "order": non_blocking_js_count
                    })
                    non_blocking_js_count += 1
                else:
                    critical_path["blocking_js"].append({
                        "url": absolute_url,
                        "relative_url": script_url,
                        "order": blocking_js_count
                    })
                    blocking_js_count += 1
        
        # <link rel="preload">を抽出（プリロードリソース）
        preload_pattern = r'<link[^>]+rel=["\']preload["\'][^>]+href=["\']([^"\']+)["\']'
        preload_matches = re.finditer(preload_pattern, html_content, re.IGNORECASE)
        for i, match in enumerate(preload_matches):
            preload_url = match.group(1)
            absolute_url = resolve_url(preload_url, base_url)
            
            # as属性を取得（リソースタイプ）
            as_match = re.search(r'as=["\']([^"\']+)["\']', match.group(0), re.IGNORECASE)
            as_type = as_match.group(1).lower() if as_match else None
            
            critical_path["preload_resources"].append({
                "url": absolute_url,
                "relative_url": preload_url,
                "as": as_type,
                "order": i
            })
        
        # 統計情報
        critical_path["blocking_css_count"] = len(critical_path["blocking_css"])
        critical_path["blocking_js_count"] = len(critical_path["blocking_js"])
        critical_path["non_blocking_js_count"] = len(critical_path["non_blocking_js"])
        critical_path["preload_resources_count"] = len(critical_path["preload_resources"])
        
    except Exception as e:
        logging.warning(f"Critical Path extraction error: {str(e)}")
        critical_path["critical_path_analysis_error"] = str(e)
    
    return critical_path


# ======================================================================
# アセットリソースの解析
# ======================================================================
def _analyze_asset_resource(
    content: bytes,
    content_type: Optional[str],
    resource_category: Optional[str],
    url_extension: Optional[str],
) -> Dict[str, Any]:
    """
    アセットリソース（画像、CSS、JSなど）の解析
    urltypeがassetの場合に実行
    """
    asset_metrics = {
        "asset_category": resource_category,
        "asset_extension": url_extension,
    }
    
    try:
        if resource_category == "image":
            # 画像メタデータの解析（サイズ、フォーマットなど）
            asset_metrics["asset_image_size_bytes"] = len(content)
            # Content-Typeから画像フォーマットを取得
            if content_type:
                asset_metrics["asset_image_mime_type"] = content_type
            # 画像の解像度などは、実際の画像ライブラリが必要（Pillowなど）
            # ここではサイズのみ記録
        
        elif resource_category == "css":
            # CSS解析（@import、url()の抽出）
            try:
                css_content = content.decode('utf-8', errors='ignore')
                # @importの抽出
                import_pattern = r'@import\s+["\']([^"\']+)["\']'
                imports = re.findall(import_pattern, css_content, re.IGNORECASE)
                asset_metrics["asset_css_import_count"] = len(imports)
                
                # url()の抽出
                url_pattern = r'url\(["\']?([^"\'()]+)["\']?\)'
                urls = re.findall(url_pattern, css_content, re.IGNORECASE)
                asset_metrics["asset_css_url_count"] = len(urls)
                
                asset_metrics["asset_css_size_bytes"] = len(content)
            except Exception as e:
                asset_metrics["asset_css_parse_error"] = str(e)
        
        elif resource_category == "js":
            # JS解析（import、requireの抽出）
            try:
                js_content = content.decode('utf-8', errors='ignore')
                # ES6 importの抽出
                import_pattern = r'import\s+.*?from\s+["\']([^"\']+)["\']'
                imports = re.findall(import_pattern, js_content, re.IGNORECASE)
                asset_metrics["asset_js_import_count"] = len(imports)
                
                # require()の抽出
                require_pattern = r'require\(["\']([^"\']+)["\']\)'
                requires = re.findall(require_pattern, js_content, re.IGNORECASE)
                asset_metrics["asset_js_require_count"] = len(requires)
                
                asset_metrics["asset_js_size_bytes"] = len(content)
            except Exception as e:
                asset_metrics["asset_js_parse_error"] = str(e)
        
        elif resource_category == "font":
            # フォント情報
            asset_metrics["asset_font_size_bytes"] = len(content)
            if content_type:
                asset_metrics["asset_font_mime_type"] = content_type
        
        elif resource_category == "video":
            # 動画情報
            asset_metrics["asset_video_size_bytes"] = len(content)
            if content_type:
                asset_metrics["asset_video_mime_type"] = content_type
        
        else:
            # その他のアセット
            asset_metrics["asset_other_size_bytes"] = len(content)
            if content_type:
                asset_metrics["asset_other_mime_type"] = content_type
        
    except Exception as e:
        logging.warning(f"Asset resource analysis error: {str(e)}")
        asset_metrics["asset_analysis_error"] = str(e)
    
    return asset_metrics


# ======================================================================
# パフォーマンス指標データ取得
# ======================================================================
def _get_performance_metrics(
    ttfb_ms: Optional[float],
    content_length_bytes: Optional[int],
    res_headers: Dict[str, str],
    redirect_count: int = 0,
    fcp_metrics: Optional[Dict[str, Any]] = None,
    lcp_metrics: Optional[Dict[str, Any]] = None,
    asset_metrics: Optional[Dict[str, Any]] = None,
    critical_path_metrics: Optional[Dict[str, Any]] = None,
    resource_info: Optional[Dict[str, Any]] = None,
    retry_info: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    パフォーマンス指標データ取得
    Core Web Vitals / PageSpeed Insights関連のデータ
    """
    metrics = {}
    
    # TTFB (既に計測済み)
    if ttfb_ms is not None:
        metrics["ttfb_ms"] = ttfb_ms
    
    # 圧縮タイプ
    headers_lower = {k.lower(): v for k, v in res_headers.items()}
    content_encoding = headers_lower.get("content-encoding", "")
    if content_encoding:
        metrics["content_encoding"] = content_encoding
    
    # Content-Length ヘッダーの値
    content_length_header = headers_lower.get("content-length")
    if content_length_header:
        metrics["content_length_header"] = content_length_header
    
    # 実際のコンテンツサイズ
    if content_length_bytes is not None:
        metrics["content_size_bytes"] = content_length_bytes
    
    # Cache-Control ヘッダーの値
    cache_control = headers_lower.get("cache-control")
    if cache_control:
        metrics["cache_control"] = cache_control
    
    # ETag ヘッダーの値
    etag = headers_lower.get("etag")
    if etag:
        metrics["etag"] = etag
    
    # Last-Modified ヘッダーの値
    last_modified = headers_lower.get("last-modified")
    if last_modified:
        metrics["last_modified"] = last_modified
    
    # リダイレクト回数
    metrics["redirect_count"] = redirect_count
    
    # CDN検出
    cdn_headers = [
        "cf-ray",  # Cloudflare
        "x-amz-cf-id",  # AWS CloudFront
        "x-nitro-cache",  # NitroCDN (NitroPack)
        "x-nitro-cache-from",  # NitroCDN (NitroPack)
        "x-nitro-rev",  # NitroCDN (NitroPack)
        "x-rl-cache",  # RabbitLoader
        "x-rl-mode",  # RabbitLoader
        "x-rl-modified",  # RabbitLoader
        "x-rl-rule",  # RabbitLoader
        "x-azure-ref",  # Azure Front Door
        "x-azure-fdid",  # Azure Front Door
        "x-azure-clientip",  # Azure Front Door
        "x-azure-socketip",  # Azure Front Door
        "x-azure-requestchain",  # Azure Front Door
        "x-akamai-request-id",  # Akamai
        "x-cache-remote",  # Akamai
        "x-true-cache-key",  # Akamai
        "x-cache-key",  # Akamai
        "x-serial",  # Akamai
        "x-akamai-edgescape",  # Akamai
        "x-check-cacheable",  # Akamai
        "x-vercel-cache",  # Vercel
        "x-vercel-id",  # Vercel
        "x-cache",  # 一般的なキャッシュヘッダー（Akamaiでも使用される可能性あり）
        "x-served-by",  # Fastly
        "x-fastly-request-id",  # Fastly
    ]
    for header in cdn_headers:
        if header in headers_lower:
            metrics["cdn_header_name"] = header
            metrics["cdn_header_value"] = headers_lower[header]
            break
    
    # GCP CDN (Cloud CDN / Media CDN) の検出
    # Server ヘッダーの値が "Google-Edge-Cache" の場合
    server_header = headers_lower.get("server", "")
    if "google-edge-cache" in server_header.lower():
        metrics["cdn_header_name"] = "server"
        metrics["cdn_header_value"] = server_header
    
    # GCP CDN のカスタムヘッダー
    if "cdn_cache_status" in headers_lower:
        metrics["cdn_header_name"] = "cdn_cache_status"
        metrics["cdn_header_value"] = headers_lower["cdn_cache_status"]
    
    # Vercel の Server ヘッダー検出
    if "vercel" in server_header.lower():
        metrics["cdn_header_name"] = "server"
        metrics["cdn_header_value"] = server_header
    
    # Azure Front Door の Via ヘッダー検出
    via_header = headers_lower.get("via", "")
    if "azure" in via_header.lower():
        metrics["cdn_header_name"] = "via"
        metrics["cdn_header_value"] = via_header
    
    # Core Web Vitals: FCP近似値
    if fcp_metrics:
        metrics.update(fcp_metrics)
    
    # Core Web Vitals: LCP要素
    if lcp_metrics:
        metrics.update(lcp_metrics)
    
    # アセット解析結果
    if asset_metrics:
        metrics.update(asset_metrics)
    
    # Critical Path（レンダリングブロッキングリソース）
    if critical_path_metrics:
        metrics.update(critical_path_metrics)
    
    # リソース情報
    if resource_info:
        metrics["resource_urltype"] = resource_info.get("urltype")
        metrics["resource_extension"] = resource_info.get("url_extension")
        metrics["resource_category"] = resource_info.get("resource_category")
    
    # リトライ情報
    if retry_info:
        metrics["retry_attempts"] = retry_info.get("retry_attempts", 0)
        if retry_info.get("retry_delays"):
            metrics["retry_delays_ms"] = [round(d * 1000, 2) for d in retry_info.get("retry_delays", [])]
        if retry_info.get("last_error"):
            metrics["retry_last_error"] = retry_info.get("last_error")
    
    return metrics


# ======================================================================
# リトライ対象エラーの判定
# ======================================================================
def _is_retryable_error(exception: Exception, status_code: Optional[int] = None) -> bool:
    """
    リトライ対象のエラーかどうかを判定
    """
    # ネットワークエラー（ConnectionError, Timeout）
    if isinstance(exception, (requests.exceptions.ConnectionError, requests.exceptions.Timeout)):
        return True
    
    # 一時的なサーバーエラー（5xx）
    if status_code and status_code in RETRYABLE_STATUS_CODES:
        return True
    
    # HTTPError（一部の例外）
    if isinstance(exception, requests.exceptions.HTTPError):
        if status_code and status_code in RETRYABLE_STATUS_CODES:
            return True
    
    # その他のネットワーク関連エラー
    if isinstance(exception, requests.exceptions.RequestException):
        # Timeout, ConnectionError以外のRequestExceptionはリトライしない
        return False
    
    return False


# ======================================================================
# HTTPリクエスト実行（リトライ対応）
# ======================================================================
def _execute_http_request_with_retry(
    target_url: str,
    headers: Dict[str, str],
) -> Tuple[requests.Response, Dict[str, Any]]:
    """
    HTTPリクエストをリトライ付きで実行
    Returns:
        Tuple[requests.Response, Dict[str, Any]]: (レスポンス, リトライ情報)
    """
    retry_info = {
        "retry_attempts": 0,
        "retry_delays": [],
        "last_error": None,
    }
    
    last_exception = None
    last_response = None
    
    for attempt in range(MAX_RETRY_ATTEMPTS + 1):  # 初回 + リトライ回数
        try:
            response = requests.get(
                target_url,
                headers=headers,
                stream=True,
                timeout=HTTP_REQUEST_TIMEOUT,
                allow_redirects=True,
            )
            
            # HTTPステータスコードをチェック
            status_code = response.status_code
            
            # リトライ対象のステータスコードの場合
            if status_code in RETRYABLE_STATUS_CODES and attempt < MAX_RETRY_ATTEMPTS:
                last_response = response
                response.close()  # レスポンスを閉じる
                
                # バックオフ待機
                delay = RETRY_INITIAL_DELAY * (RETRY_BACKOFF_MULTIPLIER ** attempt)
                retry_info["retry_delays"].append(delay)
                time.sleep(delay)
                retry_info["retry_attempts"] += 1
                logging.warning(f"Retry attempt {retry_info['retry_attempts']} for status {status_code} after {delay}s delay")
                continue
            
            # 成功した場合
            retry_info["retry_attempts"] = attempt
            return response, retry_info
            
        except requests.exceptions.RequestException as e:
            last_exception = e
            retry_info["last_error"] = str(e)
            
            # リトライ対象のエラーかどうかを判定
            if _is_retryable_error(e) and attempt < MAX_RETRY_ATTEMPTS:
                # バックオフ待機
                delay = RETRY_INITIAL_DELAY * (RETRY_BACKOFF_MULTIPLIER ** attempt)
                retry_info["retry_delays"].append(delay)
                time.sleep(delay)
                retry_info["retry_attempts"] += 1
                logging.warning(f"Retry attempt {retry_info['retry_attempts']} for error {type(e).__name__} after {delay}s delay")
                continue
            else:
                # リトライ対象でない、または最大リトライ回数に達した場合
                raise
    
    # すべてのリトライが失敗した場合
    if last_response:
        last_response.close()
    if last_exception:
        raise last_exception
    raise requests.exceptions.RequestException("All retry attempts failed")


# ======================================================================
# フラットな結果を構築
# ======================================================================
def _build_flat_result(
    *,
    status_code: int,
    status_message: str,
    duration_ms: float,
    ttfb_ms: Optional[float] = None,
    content_length_bytes: Optional[int] = None,
    target_url: str,
    http_request_number: Optional[Any] = None,
    http_request_uuid: Optional[str] = None,
    http_request_round_id: Optional[int] = None,
    req_headers: Dict[str, str],
    res_headers: Dict[str, str],
    tls_version: Optional[str] = None,
    http_protocol_version: Optional[str] = None,
    request_start_timestamp: Optional[float] = None,
    request_end_timestamp: Optional[float] = None,
    execution_id: Optional[str] = None,
    area: Optional[str] = None,
    redirect_count: int = 0,
    fcp_metrics: Optional[Dict[str, Any]] = None,
    lcp_metrics: Optional[Dict[str, Any]] = None,
    asset_metrics: Optional[Dict[str, Any]] = None,
    critical_path_metrics: Optional[Dict[str, Any]] = None,
    resource_info: Optional[Dict[str, Any]] = None,
    retry_info: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
    # レスポンスを論理的な順序で構築（用途が似たもの、意味の関係が深いものを近くに配置）
    ordered_result = {}
    
    # エリア情報の取得（環境変数から取得）
    if area is None:
        azfunc_area = os.environ.get("REGION_NAME")
        if not azfunc_area:
            raise RuntimeError("REGION_NAME environment variable is not set")
    else:
        azfunc_area = area
    
    # ======================================================================
    # 1. 基本HTTP情報（リクエスト/レスポンスの基本情報）
    # ======================================================================
    ordered_result["headers.general.status-code"] = status_code
    ordered_result["headers.general.status-message"] = status_message
    ordered_result["headers.general.request-url"] = target_url
    ordered_result["headers.general.http-request-method"] = "GET"
    
    # ======================================================================
    # 2. リクエスト識別情報（リクエストを追跡するための識別子）
    # ======================================================================
    if http_request_number is not None:
        ordered_result["eo.meta.http-request-number"] = http_request_number
    if http_request_uuid is not None:
        ordered_result["eo.meta.http-request-uuid"] = http_request_uuid
    if http_request_round_id is not None:
        ordered_result["eo.meta.http-request-round-id"] = http_request_round_id
    
    # ======================================================================
    # 3. 実行環境・タイムスタンプ情報（実行環境とタイミング情報）
    # ======================================================================
    ordered_result["eo.meta.area"] = azfunc_area
    if execution_id is not None:
        ordered_result["eo.meta.execution-id"] = execution_id
    if request_start_timestamp is not None:
        ordered_result["eo.meta.request-start-timestamp"] = request_start_timestamp
    if request_end_timestamp is not None:
        ordered_result["eo.meta.request-end-timestamp"] = request_end_timestamp
    
    # ======================================================================
    # 4. プロトコル情報（HTTP/TLSプロトコル関連）
    # ======================================================================
    # HTTPプロトコルバージョンとTLSバージョンのデフォルト値
    # 取得できない場合は"unknown"を設定（AWS Lambda版と統一）
    protocol_value = http_protocol_version or "unknown"
    tls_version_value = tls_version or "unknown"
    ordered_result["eo.meta.http-protocol-version"] = protocol_value
    ordered_result["eo.meta.tls-version"] = tls_version_value
    
    # ======================================================================
    # 5. 基本測定値（リクエスト/レスポンスの基本的な計測値）
    # ======================================================================
    ordered_result["eo.measure.duration-ms"] = round(duration_ms, 2)
    if ttfb_ms is not None:
        ordered_result["eo.measure.ttfb-ms"] = round(ttfb_ms, 2)
    if content_length_bytes is not None:
        ordered_result["eo.measure.actual-content-length"] = content_length_bytes
    
    # ======================================================================
    # 6. リソース情報（リクエスト対象リソースの種別・拡張子）
    # ======================================================================
    # パフォーマンス指標を先に取得（resource_infoが含まれるため）
    performance_metrics = _get_performance_metrics(
        ttfb_ms=ttfb_ms,
        content_length_bytes=content_length_bytes,
        res_headers=res_headers,
        redirect_count=redirect_count,
        fcp_metrics=fcp_metrics,
        lcp_metrics=lcp_metrics,
        asset_metrics=asset_metrics if "asset_metrics" in locals() else None,
        critical_path_metrics=critical_path_metrics,
        resource_info=resource_info if "resource_info" in locals() else None,
        retry_info=retry_info if "retry_info" in locals() else None,
    )
    
    # リソース情報を先に追加（resource_urltype, resource_extension, resource_category）
    if "resource_urltype" in performance_metrics:
        ordered_result["eo.performance.resource_urltype"] = performance_metrics.pop("resource_urltype")
    if "resource_extension" in performance_metrics:
        ordered_result["eo.performance.resource_extension"] = performance_metrics.pop("resource_extension")
    if "resource_category" in performance_metrics:
        ordered_result["eo.performance.resource_category"] = performance_metrics.pop("resource_category")
    
    # ======================================================================
    # 7. パフォーマンス指標（Core Web Vitals、Critical Path、キャッシュなど）
    # ======================================================================
    # TTFB関連（基本測定値の近くに配置）
    if "ttfb_ms" in performance_metrics:
        ordered_result["eo.performance.ttfb_ms"] = performance_metrics.pop("ttfb_ms")
    
    # リダイレクト情報
    if "redirect_count" in performance_metrics:
        ordered_result["eo.performance.redirect_count"] = performance_metrics.pop("redirect_count")
    
    # FCP関連（First Contentful Paint）
    fcp_keys = [k for k in performance_metrics.keys() if k.startswith("fcp_")]
    for key in sorted(fcp_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # LCP関連（Largest Contentful Paint）
    lcp_keys = [k for k in performance_metrics.keys() if k.startswith("lcp_")]
    for key in sorted(lcp_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # Critical Path関連（レンダリングブロッキングリソース）
    critical_path_keys = [k for k in performance_metrics.keys() if k.startswith(("blocking_", "non_blocking_", "preload_", "critical_path_"))]
    for key in sorted(critical_path_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # アセット解析結果
    asset_keys = [k for k in performance_metrics.keys() if k.startswith("asset_")]
    for key in sorted(asset_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # コンテンツ情報（サイズ、エンコーディング、キャッシュ）
    content_keys = ["content_size_bytes", "content_length_header", "content_encoding", "html_encoding"]
    for key in content_keys:
        if key in performance_metrics:
            ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    cache_keys = [k for k in performance_metrics.keys() if k in ("cache_control", "etag", "last_modified")]
    for key in sorted(cache_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # CDN情報
    cdn_keys = [k for k in performance_metrics.keys() if k.startswith("cdn_")]
    for key in sorted(cdn_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # リトライ情報（エラーハンドリング関連）
    retry_keys = [k for k in performance_metrics.keys() if k.startswith("retry_")]
    for key in sorted(retry_keys):
        ordered_result[f"eo.performance.{key}"] = performance_metrics.pop(key)
    
    # その他のパフォーマンス指標（残りをアルファベット順で追加）
    remaining_perf_keys = sorted(performance_metrics.keys())
    for key in remaining_perf_keys:
        ordered_result[f"eo.performance.{key}"] = performance_metrics[key]
    
    # ======================================================================
    # 8. セキュリティ情報（セキュリティヘッダー関連）
    # ======================================================================
    security_metrics = _analyze_security_headers(res_headers, target_url)
    for key in sorted(security_metrics.keys()):
        ordered_result[f"eo.security.{key}"] = security_metrics[key]
    
    # ======================================================================
    # 9. リクエストヘッダー（クライアントが送信したヘッダー）
    # ======================================================================
    for key in sorted(req_headers.keys()):
        ordered_result[f"headers.request-headers.{key.lower()}"] = req_headers[key]
    
    # ======================================================================
    # 10. レスポンスヘッダー（サーバーが返したヘッダー）
    # ======================================================================
    for key in sorted(res_headers.keys()):
        ordered_result[f"headers.response-headers.{key.lower()}"] = res_headers[key]
    
    return ordered_result


# ======================================================================
# メイン関数
# ======================================================================
@app.route(route="requestengine_func", auth_level=func.AuthLevel.FUNCTION, methods=["GET", "POST"])
def requestengine_func(req: func.HttpRequest) -> func.HttpResponse:
    start_time = time.time()

    try:
        # ==================================================================
        # n8nのHttpRequestノード(リクエストエンジンノード)からリクエスト受信 
        # (Receive from n8n HttpRequest node (Request Engine node))
        # ==================================================================
        # JSON パースとデータ抽出
        try:
            body_json = req.get_json() or {}
        except Exception:
            body_json = {}
        # ==================================================================
        # データ構造の正規化（AWS Lambda / Cloudflare Workers / GCP Cloud Runと同様）
        # ==================================================================
        data = body_json.get("data") if isinstance(body_json, dict) else body_json
        if not isinstance(data, dict):
            data = {}

        # ==================================================================
        # リクエストデータの抽出
        # ==================================================================
        target_url = data.get("targetUrl") or ""
        n8n_requestsecret_token = data.get("tokenCalculatedByN8n")
        http_request_number = data.get("httpRequestNumber")
        http_request_uuid = data.get("httpRequestUUID")
        http_request_round_id = data.get("httpRequestRoundID")
        urltype = data.get("urltype")
        input_headers = data.get("headers") if isinstance(data.get("headers"), dict) else {}

        # User-Agentヘッダーの取得
        # リクエストボディのdata.headersのみを使用（フォールバックなし）
        ua_from_request_headers = input_headers.get("User-Agent") or ""

        # ==================================================================
        # URLタイプと拡張子に基づくリソース種別の判定
        # ==================================================================
        resource_info = _determine_resource_type(urltype, target_url)

        # ==================================================================
        # URL 検証
        # ==================================================================
        if not target_url:
            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000
            error_resource_info = _determine_resource_type(urltype, "")
            execution_id = _get_execution_id()
            result = _build_flat_result(
                status_code=400,
                status_message="MISSING_URL",
                duration_ms=duration_ms,
                target_url="",
                http_request_number=http_request_number,
                http_request_uuid=http_request_uuid,
                http_request_round_id=http_request_round_id,
                req_headers={},
                res_headers={},
                request_start_timestamp=start_time,
                request_end_timestamp=end_time,
                execution_id=execution_id,
                resource_info=error_resource_info,
            )
            return func.HttpResponse(
                json.dumps(result),
                status_code=400,
                mimetype="application/json",
            )

        # ==================================================================
        # n8n生成トークンとAzure Key Vault(コンテナー)のシークレット AZFUNC-REQUEST-SECRET から生成したトークンの照合
        # ==================================================================
        # Azure Key Vault(コンテナー)からシークレットを取得
        # 注意: シークレットは1回だけ取得され、以降はキャッシュされる
        try:
            kv_request_secret = _get_kv_request_secret()
            tokenCalculatedByCloudSecret = _calc_token(target_url, kv_request_secret)  # SHA-256(url + リクエストシークレット)
            if n8n_requestsecret_token == tokenCalculatedByCloudSecret:
                # トークン検証成功: 処理を続行
                pass
            else:
                end_time = time.time()
                duration_ms = (end_time - start_time) * 1000
                execution_id = _get_execution_id()
                result = _build_flat_result(
                    status_code=401,
                    status_message="INVALID_TOKEN",
                    duration_ms=duration_ms,
                    target_url=target_url,
                    http_request_number=http_request_number,
                    http_request_uuid=http_request_uuid,
                    http_request_round_id=http_request_round_id,
                    req_headers={},
                    res_headers={},
                    request_start_timestamp=start_time,
                    request_end_timestamp=end_time,
                    execution_id=execution_id,
                    resource_info=resource_info,
                )
                return func.HttpResponse(
                    json.dumps(result),
                    status_code=401,
                    mimetype="application/json",
                )
        except Exception as e:
            logging.error(f"Request Secret validation failed: {str(e)}")
            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000
            execution_id = _get_execution_id()
            result = _build_flat_result(
                status_code=500,
                status_message="SECRET_FETCH_FAILED",
                duration_ms=duration_ms,
                target_url=target_url,
                http_request_number=http_request_number,
                http_request_uuid=http_request_uuid,
                http_request_round_id=http_request_round_id,
                req_headers={},
                res_headers={},
                request_start_timestamp=start_time,
                request_end_timestamp=end_time,
                execution_id=execution_id,
                resource_info=resource_info,
            )
            return func.HttpResponse(
                json.dumps(result),
                status_code=500,
                mimetype="application/json",
            )

        # ==================================================================
        # リクエストヘッダー準備
        # ==================================================================
        headers = dict(input_headers)
        # User-Agentが指定されている場合は設定（n8nワークフロー175番ノードで必ず付与される）
        if ua_from_request_headers:
            headers.setdefault("User-Agent", ua_from_request_headers)
        # EO識別ヘッダーを追加（Request Engineの識別用）
        headers[EO_HEADER_NAME] = EO_HEADER_VALUE

        # ==================================================================
        # HTTP リクエスト実行（リトライ対応）
        # ==================================================================
        # TTFB (Time To First Byte) 計測
        # 定義: HTTPリクエストを送信してから、サーバーから最初のバイト（レスポンスヘッダー）を受信するまでの時間
        # 含まれる要素: DNSルックアップ、TCP接続確立、TLSハンドシェイク（HTTPSの場合）、サーバー内部処理、ネットワーク遅延
        # 計測方法: リクエスト送信直前の時刻を記録し、withブロックに入った時点（レスポンスヘッダー受信完了時 = 最初のバイト受信時）で計測
        http_request_start_time = time.time()
        try:
            # ==================================================================
            # WarmupターゲットURLにHTTPリクエスト送信
            # (Send to Request for Warmup Target URL)
            # ==================================================================
            # リトライ対応のHTTPリクエスト実行
            response, retry_info = _execute_http_request_with_retry(
                target_url,
                headers,
            )
            
            with response:

                # ==================================================================
                # WarmupResultData受信
                # (Recieve WarmupResultData from Target URL)
                # ==================================================================

                # ==================================================================
                # TTFB計測: withブロックに入った時点 = レスポンスヘッダー受信完了時点（最初のバイト受信時）
                # ==================================================================
                ttfb_end = time.time()
                ttfb_ms = (ttfb_end - http_request_start_time) * 1000

                # ==================================================================
                # HTTPプロトコルバージョン取得
                # ==================================================================
                http_protocol_version = _get_http_protocol_version(response)

                # ==================================================================
                # TLSバージョン取得
                # ==================================================================
                tls_version = _get_tls_version(response, target_url)

                # ==================================================================
                # レスポンスヘッダー取得
                # ==================================================================
                res_headers = dict(response.headers)

                # ==================================================================
                # リダイレクト回数の取得
                # ==================================================================
                redirect_count = len(response.history) if hasattr(response, 'history') else 0

                # ==================================================================
                # Core Web Vitals: FCP近似値の計測（main_documentの場合のみ重点的に計測）
                # ==================================================================
                # パフォーマンス最適化: 必要最小限のコンテンツのみ読み込む
                fcp_metrics = {}
                full_content = b""
                
                if resource_info.get("needs_fcp_measurement", False):
                    # main_documentの場合はFCP計測を実行（全コンテンツを読み込む）
                    fcp_metrics, full_content = _measure_fcp_approximation(response, http_request_start_time)
                    # メモリ保護: 5MBを超える場合は解析をスキップ
                    if len(full_content) > MAX_CONTENT_SIZE_FOR_ANALYSIS:
                        logging.warning(f"Content size exceeds {MAX_CONTENT_SIZE_FOR_ANALYSIS} bytes, truncating for analysis")
                        full_content = full_content[:MAX_CONTENT_SIZE_FOR_ANALYSIS]
                else:
                    # assetの場合は必要最小限のみ読み込む
                    if resource_info.get("needs_asset_analysis", False):
                        # アセット解析が必要な場合（CSS/JS解析など）
                        full_content = response.content
                        # メモリ保護: 5MBを超える場合は解析をスキップ
                        if len(full_content) > MAX_CONTENT_SIZE_FOR_ANALYSIS:
                            logging.warning(f"Asset content size exceeds {MAX_CONTENT_SIZE_FOR_ANALYSIS} bytes, truncating for analysis")
                            full_content = full_content[:MAX_CONTENT_SIZE_FOR_ANALYSIS]
                    else:
                        # アセット解析が不要な場合（画像など）、サイズ計測のみ
                        # Warmup目的ならボディを読み込まない（Content-Lengthヘッダーからサイズを取得）
                        full_content = b""  # ボディを読み込まない（Warmup目的）
                
                # ==================================================================
                # ボディサイズ取得
                # ==================================================================
                if full_content:
                    content_length = len(full_content)
                else:
                    # full_contentが空の場合は、Content-Lengthヘッダーから取得
                    content_length_header = res_headers.get("Content-Length") or res_headers.get("content-length")
                    if content_length_header:
                        try:
                            content_length = int(content_length_header)
                        except (ValueError, TypeError):
                            content_length = 0
                    else:
                        # Content-Lengthヘッダーがない場合は実際に読み込む必要がある
                        # ただし、Warmup目的なら最小限（ヘッダーだけ）で十分
                        content_length = 0
                
                # ==================================================================
                # Core Web Vitals: LCP要素の抽出（main_documentの場合のみ）
                # ==================================================================
                content_type = res_headers.get("Content-Type", res_headers.get("content-type", ""))
                lcp_metrics = _extract_lcp_elements(
                    full_content,
                    content_type,
                    target_url,
                    urltype=urltype,
                    url_extension=resource_info.get("url_extension"),
                )
                
                # ==================================================================
                # Critical Path（レンダリングブロッキングリソース）の抽出（main_documentの場合のみ）
                # ==================================================================
                critical_path_metrics = _extract_critical_path(
                    full_content,
                    content_type,
                    target_url,
                    urltype=urltype,
                    url_extension=resource_info.get("url_extension"),
                )
                
                # ==================================================================
                # アセット解析（assetの場合、コンテンツが読み込まれている場合のみ）
                # ==================================================================
                asset_metrics = {}
                if resource_info.get("needs_asset_analysis", False) and full_content:
                    asset_metrics = _analyze_asset_resource(
                        full_content,
                        content_type,
                        resource_info.get("resource_category"),
                        resource_info.get("url_extension"),
                    )
                elif resource_info.get("needs_asset_analysis", False) and not full_content:
                    # コンテンツが読み込まれていない場合は解析をスキップ（Warmup目的）
                    asset_metrics["asset_analysis_skipped"] = True
                    asset_metrics["asset_skip_reason"] = "content_not_loaded_for_warmup"

                # ==================================================================
                # 結果構築
                # ==================================================================
                end_time = time.time()
                duration_ms = (end_time - start_time) * 1000
                execution_id = _get_execution_id()
                result = _build_flat_result(
                    status_code=response.status_code,
                    status_message=response.reason or "OK",
                    duration_ms=duration_ms,
                    ttfb_ms=ttfb_ms,
                    content_length_bytes=content_length,
                    target_url=target_url,
                    http_request_number=http_request_number,
                    http_request_uuid=http_request_uuid,
                    http_request_round_id=http_request_round_id,
                    req_headers=headers,
                    res_headers=res_headers,
                    tls_version=tls_version,
                    http_protocol_version=http_protocol_version,
                    request_start_timestamp=http_request_start_time,
                    request_end_timestamp=end_time,
                    execution_id=execution_id,
                    redirect_count=redirect_count,
                    fcp_metrics=fcp_metrics,
                    lcp_metrics=lcp_metrics,
                    asset_metrics=asset_metrics,
                    critical_path_metrics=critical_path_metrics,
                    resource_info=resource_info,
                )
                # ==================================================================
                # n8nのHttpRequestノード(リクエストエンジンノード)にWarmup結果データを返す
                # (Reply to n8n HttpRequest node (Request Engine node) with WarmupResultData)
                # ==================================================================
                return func.HttpResponse(
                    json.dumps(result),
                    status_code=200,
                    mimetype="application/json",
                )

        # ==================================================================
        # エラーハンドリング
        # ==================================================================
        except requests.exceptions.RequestException as e:
            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000
            execution_id = _get_execution_id()
            # エラー時もresource_infoを取得（可能な場合）
            error_resource_info = resource_info if "resource_info" in locals() else _determine_resource_type(urltype if "urltype" in locals() else None, target_url)
            result = _build_flat_result(
                status_code=500,
                status_message=f"Request failed: {str(e)}",
                duration_ms=duration_ms,
                target_url=target_url,
                http_request_number=http_request_number,
                http_request_uuid=http_request_uuid,
                http_request_round_id=http_request_round_id,
                req_headers=headers,
                res_headers={},
                request_start_timestamp=http_request_start_time if "http_request_start_time" in locals() else None,
                request_end_timestamp=end_time,
                execution_id=execution_id,
                redirect_count=0,
                critical_path_metrics=None,
                resource_info=error_resource_info,
                retry_info=retry_info if "retry_info" in locals() else None,
            )
            return func.HttpResponse(
                json.dumps(result),
                status_code=500,
                mimetype="application/json",
            )

    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        end_time = time.time()
        duration_ms = (end_time - start_time) * 1000
        execution_id = _get_execution_id()
        # エラー時もresource_infoを取得（可能な場合）
        error_target_url = target_url if "target_url" in locals() else ""
        error_urltype = urltype if "urltype" in locals() else None
        error_resource_info = _determine_resource_type(error_urltype, error_target_url) if error_target_url else None
        result = _build_flat_result(
            status_code=500,
            status_message=f"Internal error: {str(e)}",
            duration_ms=duration_ms,
            target_url=error_target_url,
            http_request_number=http_request_number if "http_request_number" in locals() else None,
            http_request_uuid=http_request_uuid if "http_request_uuid" in locals() else None,
            http_request_round_id=http_request_round_id if "http_request_round_id" in locals() else None,
            req_headers=input_headers if "input_headers" in locals() else {},
            res_headers={},
            request_start_timestamp=start_time if "start_time" in locals() else None,
            request_end_timestamp=end_time,
            execution_id=execution_id,
            redirect_count=0,
            critical_path_metrics=None,
            resource_info=error_resource_info,
            retry_info=None,  # 予期しないエラーの場合はリトライ情報なし
        )
        return func.HttpResponse(
            json.dumps(result),
            status_code=500,
            mimetype="application/json",
        )

